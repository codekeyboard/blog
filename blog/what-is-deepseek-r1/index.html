<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">What is DeepSeek-R1? | M. Saim</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="What is DeepSeek-R1? | M. Saim"><meta data-rh="true" name="description" content="If you’ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI’s o1 model showed that when LLMs are trained to do the same—by using more compute during inference—they get significantly better at solving reasoning tasks like mathematics, coding, and logic."><meta data-rh="true" property="og:description" content="If you’ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI’s o1 model showed that when LLMs are trained to do the same—by using more compute during inference—they get significantly better at solving reasoning tasks like mathematics, coding, and logic."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-01-30T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://example.com"><meta data-rh="true" property="article:tag" content="AI,deep learning,reinforcement learning,reasoning models"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1","mainEntityOfPage":"https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1","url":"https://your-docusaurus-site.example.com/blog/what-is-deepseek-r1","headline":"What is DeepSeek-R1?","name":"What is DeepSeek-R1?","description":"If you’ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI’s o1 model showed that when LLMs are trained to do the same—by using more compute during inference—they get significantly better at solving reasoning tasks like mathematics, coding, and logic.","datePublished":"2025-01-30T00:00:00.000Z","author":{"@type":"Person","name":"Saim","description":"AI Engineer","url":"https://example.com","image":"https://github.com/slorber.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://your-docusaurus-site.example.com/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="M. Saim RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="M. Saim Atom Feed"><link rel="stylesheet" href="/assets/css/styles.ebd0f903.css">
<script src="/assets/js/runtime~main.77e54de4.js" defer="defer"></script>
<script src="/assets/js/main.b017a936.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://github.com/slorber.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">M. Saim</b></a><a class="navbar__item navbar__link" href="/docs/intro">Tutorial</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/what-is-deepseek-r1">What is DeepSeek-R1?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/deepseek-r1-and-its-impact-on-reasoning-models">Revolutionizing Reasoning Models with Reinforcement Learning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/from-data-scientist-to-ai-developer">From Data Scientist to AI Developer: Lessons Building a Generative AI Web App in 2023</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/essential-nlp-tools">Essential NLP Tools and Libraries</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/comparing-rag-and-fine-tuning">Comparing RAG and Fine-Tuning</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">What is DeepSeek-R1?</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-01-30T00:00:00.000Z">January 30, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/blog/authors/siam"><img class="avatar__photo authorImage_XqGP" src="https://github.com/slorber.png" alt="Saim"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/blog/authors/siam"><span class="authorName_yefp">Saim</span></a></div><small class="authorTitle_nd0D" title="AI Engineer">AI Engineer</small><div class="authorSocials_rSDt"><a href="https://x.com/saim_ki_tweets" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="X"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="none" viewBox="0 0 1200 1227" style="--dark:#000;--light:#fff" class="authorSocialLink_owbf xSvg_y3PF"><path d="M714.163 519.284 1160.89 0h-105.86L667.137 450.887 357.328 0H0l468.492 681.821L0 1226.37h105.866l409.625-476.152 327.181 476.152H1200L714.137 519.284h.026ZM569.165 687.828l-47.468-67.894-377.686-540.24h162.604l304.797 435.991 47.468 67.894 396.2 566.721H892.476L569.165 687.854v-.026Z"></path></svg></a><a href="https://www.linkedin.com/in/muhammad-saim-81441b229/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" class="authorSocialLink_owbf"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453" fill="#0A66C2"></path></svg></a><a href="https://github.com/codecsaim" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialLink_owbf githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a><a href="https://https://example.com" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="newsletter"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="authorSocialLink_owbf"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M3 12a9 9 0 1 0 18 0a9 9 0 0 0 -18 0"></path><path d="M3.6 9h16.8"></path><path d="M3.6 15h16.8"></path><path d="M11.5 3a17 17 0 0 0 0 18"></path><path d="M12.5 3a17 17 0 0 1 0 18"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>If you’ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI’s o1 model showed that when LLMs are trained to do the same—by using more compute during inference—they get significantly better at solving reasoning tasks like mathematics, coding, and logic.</p>
<p>However, the recipe behind OpenAI’s reasoning models has been a well-kept secret. That is, until last week, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet (and the stock market!).</p>
<p>Besides performing as well or better than o1, the DeepSeek-R1 release was accompanied by a detailed tech report that outlined the key steps of their training recipe. This recipe involved several innovations, most notably the application of pure reinforcement learning to teach a base language model how to reason without any human supervision. As shown in the figure below, making a powerful reasoning model is now very simple if you have access to a capable base model and a high-quality data mixture:</p>
<p><img decoding="async" loading="lazy" alt="DeepSeek-R1 Diagram" src="/assets/images/image-d583a71c0fca94e85e547e87722d6c74.png" width="1162" height="501" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deepseek-r1-training-pipeline">DeepSeek-R1 Training Pipeline<a href="#deepseek-r1-training-pipeline" class="hash-link" aria-label="Direct link to DeepSeek-R1 Training Pipeline" title="Direct link to DeepSeek-R1 Training Pipeline">​</a></h2>
<p>However, the DeepSeek-R1 release leaves open several questions about:</p>
<ul>
<li><strong>Data collection</strong>: How were the reasoning-specific datasets curated?</li>
<li><strong>Model training</strong>: No training code was released by DeepSeek, so it is unknown which hyperparameters work best and how they differ across different model families and scales.</li>
<li><strong>Scaling laws</strong>: What are the compute and data trade-offs in training reasoning models?</li>
</ul>
<p>These questions prompted us to launch the <strong>Open-R1</strong> project, an initiative to systematically reconstruct DeepSeek-R1’s data and training pipeline, validate its claims, and push the boundaries of open reasoning models. By building Open-R1, we aim to provide transparency on how reinforcement learning can enhance reasoning, share reproducible insights with the open-source community, and create a foundation for future models to leverage these techniques.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-did-they-do-it">How Did They Do It?<a href="#how-did-they-do-it" class="hash-link" aria-label="Direct link to How Did They Do It?" title="Direct link to How Did They Do It?">​</a></h2>
<p>DeepSeek-R1 is a reasoning model built on the foundation of <strong>DeepSeek-V3</strong>. Like any good reasoning model, it starts with a strong base model, and DeepSeek-V3 is exactly that. This <strong>671B Mixture of Experts (MoE)</strong> model performs on par with heavyweights like <strong>Sonnet 3.5 and GPT-4o</strong>. What’s especially impressive is how cost-efficient it was to train—just <strong>$5.5M</strong>—thanks to architectural changes like:</p>
<ul>
<li><strong>Multi Token Prediction (MTP)</strong></li>
<li><strong>Multi-Head Latent Attention (MLA)</strong></li>
<li><strong>A LOT of hardware optimization</strong></li>
</ul>
<p>DeepSeek also introduced two models: <strong>DeepSeek-R1-Zero</strong> and <strong>DeepSeek-R1</strong>, each with a distinct training approach.</p>
<ul>
<li>
<p><strong>DeepSeek-R1-Zero</strong> skipped supervised fine-tuning altogether and relied entirely on reinforcement learning (RL), using <strong>Group Relative Policy Optimization (GRPO)</strong> to make the process more efficient. A simple reward system guided the model, providing feedback based on the accuracy and structure of its answers. This approach helped the model develop useful reasoning skills, such as breaking problems into steps and verifying its own outputs. However, its responses often lacked clarity and were difficult to read.</p>
</li>
<li>
<p><strong>DeepSeek-R1</strong> improved upon this by starting with a <strong>&quot;cold start&quot; phase</strong>, fine-tuning on a small set of carefully crafted examples to enhance clarity and readability. From there, it underwent additional RL and refinement steps, including <strong>rejecting low-quality outputs using human preference-based and verifiable rewards</strong>. This resulted in a model that not only reasons well but also produces polished and consistent answers.</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="DeepSeek Training Process" src="/assets/images/image-1-f0b7c61ca3b2ef29a6ca54fc86eb379d.png" width="3412" height="1694" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-r1-the-missing-pieces">Open-R1: The Missing Pieces<a href="#open-r1-the-missing-pieces" class="hash-link" aria-label="Direct link to Open-R1: The Missing Pieces" title="Direct link to Open-R1: The Missing Pieces">​</a></h2>
<p>The release of DeepSeek-R1 is an amazing boon for the community, but they didn’t release everything—although the model weights are open, the <strong>datasets and training code</strong> are not 😢.</p>
<p>The goal of <strong>Open-R1</strong> is to build these last missing pieces so that the whole research and industry community can build similar or better models using these recipes and datasets. And by doing this in the open, <strong>everybody in the community can contribute!</strong></p>
<p>As shown in the figure below, here’s our plan of attack:</p>
<ol>
<li><strong>Replicate the R1-Distill models</strong> by distilling a high-quality reasoning dataset from DeepSeek-R1.</li>
<li><strong>Replicate the pure RL pipeline</strong> that DeepSeek used to create R1-Zero. This will involve curating new, large-scale datasets for math, reasoning, and code.</li>
<li><strong>Show we can go from base model → SFT → RL</strong> via multi-stage training.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Open-R1 Plan" src="/assets/images/image-2-75167e81225a0fa432aa6e48f85a10ef.png" width="944" height="1060" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-involved">Get Involved<a href="#get-involved" class="hash-link" aria-label="Direct link to Get Involved" title="Direct link to Get Involved">​</a></h2>
<p>The synthetic datasets will allow everyone to fine-tune existing or new LLMs into reasoning models by simply fine-tuning on them. The training recipes involving RL will serve as a starting point for anyone to build similar models from scratch and will allow researchers to build even more advanced methods on top.</p>
<p>We <strong>don’t want to stop at math datasets</strong>. There’s a lot of potential in exploring other areas, obvious ones like <strong>code</strong>, but also <strong>scientific fields such as medicine</strong>, where reasoning models could have a significant impact.</p>
<p>This initiative isn’t just about replicating results—it’s about <strong>sharing insights with the community</strong>. By documenting what works, what doesn’t, and why, we hope to save others from wasting time and compute on unproductive paths.</p>
<p>If this sounds interesting, we’d love your help! Whether it’s contributing code, joining discussions on Hugging Face, or sharing feedback, there are plenty of ways to get involved.</p>
<p>Let’s build this together! 🚀</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="AI tag description" class="tag_zVej tagRegular_sFm0" href="/blog/tags/AI">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/deep-learning">deep learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/reinforcement-learning">reinforcement learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/reasoning-models">reasoning models</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-30-What-is-DeepSeek.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/deepseek-r1-and-its-impact-on-reasoning-models"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Revolutionizing Reasoning Models with Reinforcement Learning</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#deepseek-r1-training-pipeline" class="table-of-contents__link toc-highlight">DeepSeek-R1 Training Pipeline</a></li><li><a href="#how-did-they-do-it" class="table-of-contents__link toc-highlight">How Did They Do It?</a></li><li><a href="#open-r1-the-missing-pieces" class="table-of-contents__link toc-highlight">Open-R1: The Missing Pieces</a></li><li><a href="#get-involved" class="table-of-contents__link toc-highlight">Get Involved</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>